# -*- coding: utf-8 -*-
"""ex2_noy_gal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v7Dl4t76cTLEc8XRyXNI5KF8jFQFvu_f

# Importing
"""

import numpy as np
import torch
from torch import optim
import torch.nn as nn
import timeit
import matplotlib.pyplot as plt
import urllib.request

"""# GPU availablity"""

# Function to check GPU availability
def check_gpu():
    if torch.cuda.is_available():
        return f"GPU is available: {torch.cuda.get_device_name(0)}"
    else:
        return "GPU is not available, using CPU instead."

# Check if GPU is available
print(check_gpu())

layer_num = 2
hidden_size = 200
weight_initiation = 0.05
batch_size = 20
seq_length = 70
learning_rate = 0.7
epochs = 3
factor_epoch = 5
factor = 2
max_grad_norm = 5
ensemble_num = 5
DropOut_prob = 0.5

"""Loading the data"""

from google.colab import drive
drive.mount('/content/drive')

def data_init():
    paths = {
        'train': '/content/drive/My Drive/PTB/ptb.train.txt',
        'valid': '/content/drive/My Drive/PTB/ptb.valid.txt',
        'test': '/content/drive/My Drive/PTB/ptb.test.txt',
    }

    # Initialize dictionaries to hold the data
    data = {}

    # Loop through each path and load the data
    for key, path in paths.items():
        with open(path, 'r') as f:
            file = f.read()
            data[key] = file.split()

    # Extract unique words and create a mapping to indices
    words = sorted(set(data['train']))
    char2ind = {c: i for i, c in enumerate(words)}

    # Convert the datasets to indices based on char2ind mapping
    trn = [char2ind[c] for c in data['train']]
    vld = [char2ind[c] for c in data['valid']]
    tst = [char2ind[c] for c in data['test']]

    # Reshape the data for later use
    return np.array(trn).reshape(-1, 1), np.array(vld).reshape(-1, 1), np.array(tst).reshape(-1, 1), len(words)

def minibatch(data, batch_size, seq_length):
    """
    Prepares minibatches for training RNNs.

    Parameters:
    - data: A NumPy array of integer indices representing words.
    - batch_size: The number of sequences per batch.
    - seq_length: The length of each sequence.

    Returns:
    - A list of tuples, each containing a batch of input sequences and target sequences.
    """

    # Convert data to a PyTorch tensor of specified type
    data = torch.tensor(data, dtype=torch.int64)

    # Calculate the number of complete batches
    num_batches = data.size(0) // batch_size

    # Trim data to fit whole number of batches
    data = data[:num_batches * batch_size]

    # Reshape data to have batch_size rows
    data = data.view(batch_size, -1)

    dataset = []  # To hold the generated batches
    for i in range(0, data.size(1) - 1, seq_length):
        # Determine the effective sequence length for this batch
        seqlen = int(np.min([seq_length, data.size(1) - 1 - i]))

        # If condition to ensure we only add complete sequences
        if seqlen < data.size(1) - 1 - i:
            # Extract the input and target sequences
            x = data[:, i:i+seqlen].transpose(1, 0)
            y = data[:, i+1:i+seqlen+1].transpose(1, 0)

            # Append the (input, target) pair to the dataset
            dataset.append((x, y))

    return dataset


class Embed(nn.Module):
    def __init__(self, vocab_size, embed_size):
        """
        Initialize the Embedding layer.

        Parameters:
        - vocab_size: The size of the vocabulary.
        - embed_size: The size of each embedding vector.
        """
        super().__init__()  # Initialize the parent class (nn.Module)
        self.vocab_size = vocab_size  # Store the vocabulary size
        self.embed_size = embed_size  # Store the embedding size

        # Initialize the weights as a parameter. nn.Parameter indicates that this tensor should be considered a parameter of the model,
        # and as such, it will be automatically included in the model's list of parameters; and gradients will be computed for it.
        self.W = nn.Parameter(torch.Tensor(vocab_size, embed_size))

    def forward(self, x):
        return self.W[x]  # Index into the parameter tensor to retrieve embeddings

    def __repr__(self):
        return "Embedding(vocab: {}, embedding: {})".format(self.vocab_size, self.embed_size)




class Linear(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.W = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.b = nn.Parameter(torch.Tensor(hidden_size))

    def forward(self, x):
        z = torch.addmm(self.b, x.view(-1, x.size(2)), self.W.t())
        return z

    def __repr__(self):
        return "FC(input: {}, output: {})".format(self.input_size, self.hidden_size)

class CustomRNN(nn.Module):
    def __init__(self, vocab_size, hidden_size, layer_num, dropout, weight_initiation, rnn_type):
        super().__init__()
        self.vocab_size = vocab_size
        self.weight_initiation = weight_initiation
        self.layer_num = layer_num
        self.embed = Embed(vocab_size, hidden_size)
        self.hidden_size = hidden_size
        self.rnn_type = rnn_type
        if rnn_type == 'GRU':
            self.rnns = [nn.GRU(hidden_size, hidden_size) for i in range(layer_num)]
        else:
            self.rnns = [nn.LSTM(hidden_size, hidden_size) for i in range(layer_num)]
        self.rnns = nn.ModuleList(self.rnns)
        self.fc = Linear(hidden_size, vocab_size)
        self.dropout = nn.Dropout(p=dropout)
        self.reset_parameters()

    def reset_parameters(self):
        for param in self.parameters():
            nn.init.uniform_(param, -self.weight_initiation, self.weight_initiation)

    def state_init(self, batch_size,rnn_type):
        dev = next(self.parameters()).device
        if rnn_type == 'LSTM':
            states = [(torch.zeros(1, batch_size, layer.hidden_size, device = dev), torch.zeros(1, batch_size, layer.hidden_size, device = dev)) for layer in self.rnns]
        else:
            states = [(torch.zeros(1, batch_size, layer.hidden_size, device = dev)) for layer in self.rnns]
        return states


    def detach(self, states,rnn_type):
        if rnn_type =='GRU':
            return [h.detach() for h in states]
        else:
            return [(h.detach(), c.detach()) for (h,c) in states]

    def forward(self, x, states):
        x = self.embed(x)
        x = self.dropout(x)
        for i, rnn in enumerate(self.rnns):
            x, states[i] = rnn(x, states[i])
            x = self.dropout(x)
        scores = self.fc(x)
        return scores, states

#define loss and perplexity functions
def loss_calc(scores, y):
    # scores: the raw output scores from the model for each class, often referred to as logits.
    # y: the ground truth labels for each example in the batch.

    # Calculate the batch size based on the shape of 'y'.
    batch_size = y.size(1)
    expscores = scores.exp()
    #normalize
    probabilities = expscores / expscores.sum(1, keepdim = True)
    answerprobs = probabilities[range(len(y.reshape(-1))), y.reshape(-1)]
    # Compute the negative log likelihood loss.
    return torch.mean(-torch.log(answerprobs) * batch_size)

def perplexity(data, model,rnn_type):
    with torch.no_grad():
        losses = []
        states = model.state_init(batch_size,rnn_type)
        for x, y in data:
            scores, states = model(x, states)
            loss = loss_calc(scores, y)
            losses.append(loss.data.item()/batch_size)
    return np.exp(np.mean(losses))

def train(data, model, epochs, epoch_threshold, lr, factor, max_norm,rnn_type):
    train_data, validation_data, test_data = data
    tic = timeit.default_timer()
    total_words = 0
    #list of perplexity
    train_perplexity = []
    test_perplexity = []
    valid_perp_min = np.Inf

    print("Starting training.\n")
    for epoch in range(epochs):
        states = model.state_init(batch_size,rnn_type)
        model.train()
        if epoch > epoch_threshold:
            lr = lr / factor
        for i, (x, y) in enumerate(train_data):
            total_words += x.numel()
            model.zero_grad()
            states = model.detach(states,rnn_type)
            scores, states = model(x, states)
            loss = loss_calc(scores, y)
            loss.backward()


            with torch.no_grad():
                norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm)
                for param in model.parameters():
                    param -= lr * param.grad

            if i % (len(train_data)//10) == 0:
                toc = timeit.default_timer()

                print("batch no = {:d} / {:d}, ".format(i, len(train_data)) +
                      "train loss = {:.3f}, ".format(loss.item()/batch_size) +
                      "lr = {:.3f}, ".format(lr) +
                      "since beginning = {:d} mins, ".format(round((toc-tic)/60))
                      )
        #evaluation
        model.eval()
        val_perp = perplexity(validation_data, model,rnn_type)

        if val_perp <= valid_perp_min:
            torch.save(model.state_dict(), 'model_cifar.pt')
            valid_perp_min = val_perp

        epoch_train_perplexity = perplexity(train_data, model, rnn_type)
        epoch_test_perplexity = perplexity(test_data, model, rnn_type)
        train_perplexity.append(epoch_train_perplexity)
        test_perplexity.append(epoch_test_perplexity)
        print("Epoch : {:d} || Validation set perplexity : {:.3f}".format(epoch+1, val_perp))
        print("*************************************************\n")
    model.load_state_dict(torch.load('model_cifar.pt'))
    tst_perp = perplexity(test_data, model,rnn_type)
    print("validation preplexity : {:.3f}".format(val_perp))
    print("Training is over.")
    return train_perplexity, test_perplexity, val_perp

def Convergence(n_epochs, test_perplexity, train_perplexity, title):
    # plotting test accuracy
    plt.plot(range(n_epochs), test_perplexity, label = "Test")
    # plotting train accuracy
    plt.plot(range(n_epochs), train_perplexity, label = "Train")
    plt.xlabel('Epoch')
    # Set the y axis label of the current axis.
    plt.ylabel('Perplexity')
    # Set a title of the current axes.
    plt.title(title)
    # show a legend on the plot
    plt.legend()
    # Display a figure.
    plt.show()

def test_model(dropout,rnn_type, epochs,learning_rate,weight_initiation,seq_length,factor_epoch,factor,max_grad_norm, title):
    trn, vld, tst, vocab_size = data_init()
    trn = minibatch(trn, batch_size, seq_length)
    vld = minibatch(vld, batch_size, seq_length)
    tst = minibatch(tst, batch_size, seq_length)
    model = CustomRNN(vocab_size, hidden_size, layer_num, dropout, weight_initiation, rnn_type)
    model.to("cuda")
    final_title = title
    train_perplexity, test_perplexity,validation_preplexity = train((trn, vld, tst), model, epochs, factor_epoch, learning_rate, factor, max_grad_norm,rnn_type)

    Convergence(epochs, test_perplexity, train_perplexity, final_title)
    return (perplexity(tst,model,rnn_type), perplexity(trn,model,rnn_type), validation_preplexity), final_title

results = [test_model(dropout=0,rnn_type='LSTM', epochs=15,learning_rate=0.95,weight_initiation=0.01,seq_length=35,factor_epoch=5,factor=2,max_grad_norm=10, title="No dropout LSTM"),
           test_model(dropout=DropOut_prob, rnn_type='LSTM',epochs=80, learning_rate=0.2,weight_initiation=0.05,seq_length=35,factor_epoch=60,factor=1.2,max_grad_norm=5, title="with dropout LSTM"),
           test_model(dropout=DropOut_prob, rnn_type='GRU', epochs=100, learning_rate=0.5,weight_initiation=0.05,seq_length=35,factor_epoch=60,factor=1.2,max_grad_norm=5, title="with dropout GRU"),
           test_model(dropout=0,rnn_type='GRU',epochs=15, learning_rate=0.95,weight_initiation=0.1,seq_length=35,factor_epoch=6,factor=1.234,max_grad_norm=5,title="No dropout GRU")]

columns = ('Perplexity Test', 'Perplexity Train','Perplexity Validation')
rows = [r[1] for r in results]


# Get some pastel shades for the colors
colors = plt.cm.BuPu(np.linspace(0, 0.5, len(rows)))
cell_text = [r[0] for r in results]


fig, ax = plt.subplots()
table = plt.table(cellText=cell_text,
                      rowLabels=rows,
                      rowColours=colors,
                      colLabels=columns,
                      loc="center")
table.auto_set_font_size(False)
table.set_fontsize(15)
table.scale(2, 2)

fig.patch.set_visible(False)
ax.axis('off')
ax.axis('tight')
fig.tight_layout()

plt.show()